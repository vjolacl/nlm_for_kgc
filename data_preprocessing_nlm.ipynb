{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c748ba2-8ab2-4f96-8295-07efa5ebf675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pykeen.datasets import WN18RR, FB15k237\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pykeen.sampling.basic_negative_sampler import BasicNegativeSampler\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification,Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044c6fd-a2f1-49d8-94c1-7331fb033392",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fbe15e-bc04-454f-99b3-c74fdf494cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataset = FB15k237()\n",
    "path_ent = \"data/fb15k237/fb15k237_entity2text.txt\"\n",
    "path_rel = 'data/fb15k237/fb15k237_relation2text.txt'\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54079943-a22c-4934-a426-f2110f4fcfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 30 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 28 from 20466 triples were filtered out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FB15k237(num_entities=14505, num_relations=237, create_inverse_triples=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b8448-8e38-4318-8252-f29d1e41b40a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get train, test and validation triples  and store in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526b945c-87d7-48b4-a5dd-5c20271c6c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def triple_labels(dataset):\n",
    "    # Get triples in label form e.g. ['/m/010016', '/location/', '/m/0mr_8']\n",
    "    train = dataset.training.triples\n",
    "    test = dataset.testing.triples\n",
    "    val= dataset.validation.triples\n",
    "    \n",
    "    return train, test, val\n",
    "\n",
    "def triple_ids(dataset):\n",
    "    # Get triples in ID form e.g. [0, 120, 13647]\n",
    "    train = dataset.training.mapped_triples\n",
    "    test = dataset.testing.mapped_triples\n",
    "    val = dataset.validation.mapped_triples\n",
    "    \n",
    "    return train, test, val\n",
    "    \n",
    "def neg_sampling(sampler, triple_ids, triple_factory):\n",
    "    \n",
    "    #Initialiaze negative sampler from pykeen\n",
    "    neg_sampler = BasicNegativeSampler(mapped_triples = triple_ids, filtered=True)\n",
    "    \n",
    "    # Compute negative samples for the given triples\n",
    "    neg_triples, filter_mask = neg_sampler.sample(triple_ids)\n",
    "    \n",
    "    # Create mask to filter out the neg_triples that are included in the initial positive triples of the KG\n",
    "    mask = np.ones(len(neg_triples), dtype=bool)\n",
    "    mask[np.where(filter_mask==False)[0]] = False\n",
    "    tensor_filtered = neg_triples[mask] # Apply the mask to remove duplicates\n",
    "    \n",
    "    # Reshape tensor to match the shape of mapped_triples from the pykeen triple factory \n",
    "    tensor_filtered = tensor_filtered.reshape(len(tensor_filtered), 3) \n",
    "    triple_labels = triple_factory.label_triples(tensor_filtered)  # enter triple ID (number) and get the triple labels   \n",
    "    \n",
    "    del tensor_filtered\n",
    "    \n",
    "    return triple_labels\n",
    "\n",
    "\n",
    "def load_ent_rel_def(dataset, path_ent, path_rel):\n",
    "    \n",
    "    if dataset==\"fb15k237\":\n",
    "        df_entity2text = pd.read_csv(path_ent, delimiter=\"\\t\", header = None, names=[\"id\", \"entity\"])\n",
    "        df_entity2text[\"segmented_entities\"] = df_entity2text[\"entity\"].str.split(' ')\n",
    "    \n",
    "        df_rel2text = pd.read_csv(path_rel, delimiter=\"\\t\", header = None, names=[\"id\", \"definition\"])\n",
    "        df_rel2text[[\"property_1_id\", \"property_2_id\"]] = df_rel2text[\"id\"].str.split('.', n=1, expand=True)\n",
    "        df_rel2text[\"property_1_id\"] = df_rel2text[\"property_1_id\"].str.replace(\"/\", \", \").str[2:]\n",
    "        df_rel2text[\"property_2_id\"] = df_rel2text[\"property_2_id\"].str.replace(\"/\", \", \").str[2:]\n",
    "        df_rel2text[\"property_1_id\"] = df_rel2text[\"property_1_id\"].str.replace(\"_\", \" \")\n",
    "        df_rel2text[\"property_2_id\"] = df_rel2text[\"property_2_id\"].str.replace(\"_\", \" \")\n",
    "        \n",
    "    elif dataset==\"wn18rr\":\n",
    "        print(\"Not implemented yet\")\n",
    "    \n",
    "    else: \n",
    "        print(\"Only datasets 'fb1k237' and 'wn18rr' are supported\")\n",
    "        \n",
    "    return df_entity2text, df_rel2text\n",
    "   \n",
    "    \n",
    "\n",
    "def triple_def(df_entity2text, df_rel2text, triples, target):\n",
    "    \n",
    "    df = pd.DataFrame(triples, columns=['head', 'rel', 'tail'])\n",
    "    \n",
    "    df['head_label'] = df['head'].map(df_entity2text.set_index('id')['entity'])\n",
    "    df['rel_label'] = df['rel'].map(df_rel2text.set_index('id')['definition'])\n",
    "    df['tail_label'] = df['tail'].map(df_entity2text.set_index('id')['entity'])\n",
    "    \n",
    "    if target==\"pos\":\n",
    "        df[\"target\"] = 1\n",
    "    elif target==\"neg\": \n",
    "        df[\"target\"] = 0\n",
    "    else: \n",
    "        print(\"This parameter can only be either 'pos' or 'neg'\")\n",
    "        \n",
    "    if df.isnull().values.any():\n",
    "        print(\"Dataframe contains nan values, review input\")\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def merge_pos_neg_triples(pos_triples, neg_triples):\n",
    "    \n",
    "    # Concatenate the positive and negative triples in the dataframe and shuffle the data \n",
    "    df = pd.concat([pos_triples, neg_triples])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.sample(frac = 1, random_state=5)\n",
    "    \n",
    "    df[\"triple\"]= df[\"head_label\"]+ \", \" + df[\"rel_label\"] + \", \" + df[\"tail_label\"]\n",
    "    \n",
    "    #Store triples in a dictionary along with the labels\n",
    "    output = { \"triples\": df[\"triple\"].tolist(), \"labels\": df[\"target\"].tolist()}\n",
    "    \n",
    "    return output\n",
    "\n",
    "class RobertaInput(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def post_tokenizer(tokenized_triples):\n",
    "    for j in range(len(tokenized_triples['input_ids'])):\n",
    "        item = tokenized_triples['input_ids'][j]\n",
    "        tokenized_triples['input_ids'][j]=[2 if item[i]==6 else item[i] for i in range(len(item))] \n",
    "    return tokenized_triples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89251a1a-0b70-4fa3-be40-d4fe432824fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 30 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 28 from 20466 triples were filtered out\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "You're trying to map triples with 9 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 9 from 17535 triples were filtered out\n",
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n",
      "You're trying to map triples with 30 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 28 from 20466 triples were filtered out\n",
      "You're trying to map triples with 9 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 9 from 17535 triples were filtered out\n"
     ]
    }
   ],
   "source": [
    "train_label, test_label, val_label =  triple_labels(FB15k237())\n",
    "train_ids, test_ids, val_ids =  triple_ids(FB15k237())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005fe737-e7a1-45ae-80d3-935da02af629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're trying to map triples with 9 entities and 0 relations that are not in the training set. These triples will be excluded from the mapping.\n",
      "In total 9 from 17535 triples were filtered out\n"
     ]
    }
   ],
   "source": [
    "neg_train_triple_labels = neg_sampling(BasicNegativeSampler, train_ids, dataset.training)\n",
    "neg_test_triple_labels = neg_sampling(BasicNegativeSampler, test_ids, dataset.testing)\n",
    "neg_val_triple_labels = neg_sampling(BasicNegativeSampler, val_ids, dataset.validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b6eb82-6939-42cc-b6e7-50df4d202444",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17472"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_val_triple_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13c4404-cab8-4fdc-8266-3cc737909db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ent, df_rel = load_ent_rel_def(\"fb15k237\", path_ent, path_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a269e4-5054-46cb-a0e2-b50f8832763a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_triple_def_pos = triple_def(df_ent, df_rel, train_label, \"pos\")\n",
    "train_triple_def_neg = triple_def(df_ent, df_rel, neg_train_triple_labels , \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e42c45-c277-452d-97c4-eaee3915fdf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_triple_def_pos = triple_def(df_ent, df_rel, test_label , \"pos\")\n",
    "test_triple_def_neg = triple_def(df_ent, df_rel, neg_test_triple_labels , \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a52eb13-de7a-406e-a595-81bf4906a693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_triple_def_pos = triple_def(df_ent, df_rel, val_label , \"pos\")\n",
    "val_triple_def_neg = triple_def(df_ent, df_rel, neg_val_triple_labels , \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e733a02d-0398-4264-96f0-4fc9fe486c66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17472"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_triple_def_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d352dad-4172-4dd2-9db6-a6f43f709660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_triples = merge_pos_neg_triples(train_triple_def_pos, train_triple_def_neg)\n",
    "test_triples = merge_pos_neg_triples(test_triple_def_pos, test_triple_def_neg)\n",
    "val_triples = merge_pos_neg_triples(val_triple_def_pos, val_triple_def_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1676ea0-6d08-472e-bf27-b8cd0fe3a20d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03f2fc1c-c769-48df-822b-622154e04c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(train_triples[\"triples\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c0e475f-db49-4ad6-aa41-8250ad91534a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_tokenized = tokenizer(test_triples[\"triples\"], padding=True, truncation=True)\n",
    "val_tokenized = tokenizer(val_triples[\"triples\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99282ce4-47ef-4e50-8de6-cffa46832521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tokenized = post_tokenizer(train_tokenized)\n",
    "test_tokenized = post_tokenizer(test_tokenized)\n",
    "val_tokenized = post_tokenizer(val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5123490d-d8a2-42e0-98cf-5213c20f5c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 26145,\n",
       " 625,\n",
       " 27171,\n",
       " 3683,\n",
       " 13,\n",
       " 2700,\n",
       " 8479,\n",
       " 16038,\n",
       " 154,\n",
       " 2,\n",
       " 2354,\n",
       " 2354,\n",
       " 4120,\n",
       " 11357,\n",
       " 4,\n",
       " 2354,\n",
       " 2354,\n",
       " 5757,\n",
       " 7076,\n",
       " 13,\n",
       " 2,\n",
       " 496,\n",
       " 815,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenized[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d77582c6-a4d6-4810-ad00-ca6e4b449efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = RobertaInput(train_tokenized, train_triples[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "991ad588-3de3-47e1-ab73-90e92c94861b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = RobertaInput(test_tokenized, test_triples[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "890a6c24-e260-40cb-bad2-26cc300f5f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_data = RobertaInput(val_tokenized, test_triples[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fd35dbc-6f38-45ea-9e2f-f64520d24a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_tokenized\n",
    "del test_tokenized\n",
    "del val_tokenized\n",
    "del tokenizer\n",
    "del train_triples\n",
    "del test_triples\n",
    "del val_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba00c2-d6cd-469e-b42b-4e310bc6e957",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4830ea4b-63d0-4210-8b63-9fd8b115c849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Double check if tokenizer worked out as it should \n",
    "#tokenizer.convert_ids_to_tokens(test_tokenized[\"input_ids\"][0])\n",
    "#tokenizer.convert_tokens_to_ids(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9dc249-a271-4d5d-a1e8-cf133b1cc89b",
   "metadata": {},
   "source": [
    "#### -------------------- ROBERTA for Text Classification -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c18cb54-00fb-4760-9e1e-87b5dc1b8657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bd305f7-a5f3-49c2-b709-6ddd48fb8c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1da5e13b-48b7-44eb-b780-c7fc09ec7887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf9017f4-06dd-4e5f-9f7d-b639f7da5aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define accuracy metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24114998-c9f0-4a28-b94d-a68acccde5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '07_roberta/1st_try_1epoch',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 32, \n",
    "    eval_accumulation_steps = 1,\n",
    "    per_device_eval_batch_size= 1,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    eval_steps = 3000,\n",
    "    save_steps = 3000,\n",
    "    #warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 8,\n",
    "    fp16 = False,\n",
    "    logging_dir='07_roberta/logs',\n",
    "    #dataloader_num_workers = 8,\n",
    "    run_name = 'roberta-frozen-layers-fb15k237'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3e473e8-50fd-4c68-92f2-3cec807c3e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabae2e-b625-4a87-a597-f0d16e0d792b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/aifb/ho8030/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvjolacl\u001b[0m (\u001b[33mnlm_kgc\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/pfs/data5/home/kit/aifb/ho8030/wandb/run-20230606_132029-38t32lbp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlm_kgc/huggingface/runs/38t32lbp\" target=\"_blank\">roberta-frozen-layers-fb15k237</a></strong> to <a href=\"https://wandb.ai/nlm_kgc/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='16905' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   71/16905 00:22 < 1:31:23, 3.07 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5157f83-3315-4e8e-bc8a-6176dca52f44",
   "metadata": {
    "tags": []
   },
   "source": [
    "### WN11 Dataset ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b38cc52-cd0a-4b7b-acbc-c259196ee546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_entity2text = pd.read_csv(\"data/wn11/entity2text.txt\", delimiter=\"\\t\", header = None, names=[\"id\", \"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83f770a-e154-454d-bee0-882ed693cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relation2text = pd.read_csv(\"data/wn11/relation2text.txt\", delimiter=\"\\t\", header = None, names=[\"id\", \"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65bf2e4b-9378-4d2c-98e8-d2af76a6ae75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__east_indian_1</td>\n",
       "      <td>east indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__lindesnes_1</td>\n",
       "      <td>lindesnes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__chlamydosaurus_kingi_1</td>\n",
       "      <td>chlamydosaurus kingi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                entity\n",
       "0           __east_indian_1           east indian\n",
       "1             __lindesnes_1             lindesnes\n",
       "2  __chlamydosaurus_kingi_1  chlamydosaurus kingi"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entity2text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb7c93f6-03f8-4395-bf56-84d8c2498b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train =  pd.read_csv(\"data/wn11/train.tsv\",  delimiter=\"\\t\", header = None, names=[\"head\", \"relation\", \"tail\"])\n",
    "test =  pd.read_csv(\"data/wn11/test.tsv\",  delimiter=\"\\t\", header = None, names=[\"head\", \"relation\", \"tail\"])\n",
    "dev =  pd.read_csv(\"data/wn11/dev.tsv\",  delimiter=\"\\t\", header = None, names=[\"head\", \"relation\", \"tail\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49e8d959-6ec2-4331-93d9-afe88f60f9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                head                  relation  \\\n",
      "109782              __house_of_god_1             _has_instance   \n",
      "109783             __confrontation_2                  _type_of   \n",
      "109784  __african_scented_mahogany_1                  _type_of   \n",
      "109785        __family_graminaceae_1           _member_meronym   \n",
      "109786                 __aplectrum_1           _member_holonym   \n",
      "...                              ...                       ...   \n",
      "112576                   __hamelin_1  _subordinate_instance_of   \n",
      "112577                    __anuran_1             _has_instance   \n",
      "112578                  __bring_up_2             _has_instance   \n",
      "112579                   __dreamer_3                  _type_of   \n",
      "112580        __american_red_elder_1                  _type_of   \n",
      "\n",
      "                           tail  \n",
      "109782          __conventicle_2  \n",
      "109783         __disagreement_3  \n",
      "109784             __mahogany_2  \n",
      "109785       __genus_glyceria_1  \n",
      "109786   __family_orchidaceae_1  \n",
      "...                         ...  \n",
      "112576                 __town_1  \n",
      "112577  __alytes_obstetricans_1  \n",
      "112578               __foster_2  \n",
      "112579           __daydreamer_1  \n",
      "112580                __elder_2  \n",
      "\n",
      "[2220 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = train.duplicated()\n",
    "print(train[duplicate_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "25c02071-af96-46f7-bca3-6714f5c24a37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TriplesFactory(num_entities=38194, num_relations=11, create_inverse_triples=False, num_triples=110361, path=\"/pfs/data5/home/kit/aifb/ho8030/data/wn11/train.tsv\")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ceab0759-eb63-4905-9a17-2e3fa52cabb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pykeen.triples import TriplesFactory\n",
    "training = TriplesFactory.from_path(\"data/wn11/train.tsv\")\n",
    "testing = TriplesFactory.from_path(\"data/wn11/test.tsv\")\n",
    "dev = TriplesFactory.from_path(\"data/wn11/dev.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea3fa037-016f-4ecf-9892-fa1e13065e77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['__0_1', '_similar_to', '__cardinal_4'],\n",
       "       ['__0_1', '_type_of', '__digit_1'],\n",
       "       ['__1000000000000_1', '_type_of', '__large_integer_1'],\n",
       "       ...,\n",
       "       ['__zymosis_2', '_has_instance', '__fungal_infection_1'],\n",
       "       ['__zymosis_2', '_synset_domain_topic', '__medical_specialty_1'],\n",
       "       ['__zymosis_2', '_type_of', '__infection_2']], dtype='<U68')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.label_triples(training.mapped_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "497453e9-4b84-40f6-8dc2-92bb6dd903eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TriplesFactory(num_entities=17336, num_relations=11, create_inverse_triples=False, num_triples=21034, path=\"/pfs/data5/home/kit/aifb/ho8030/data/wn11/test.tsv\")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b880fa06-eb27-4833-84d1-88f71b4e2f38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112581"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fd684-5728-46cc-861a-014d4261414c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlm_kge2",
   "language": "python",
   "name": "nlm_kge2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
